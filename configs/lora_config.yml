lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules:
    - "query"
    - "value"
    - "key"
    - "ffn"

training:
  epochs: 10
  batch_size: 8
  learning_rate: 1e-4
  weight_decay: 0
  optimizer: "adam"
  device: "cuda"
