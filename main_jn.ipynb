{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860accf7",
   "metadata": {},
   "source": [
    "# cov_vaccine_degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094d2617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Configuration (Multimodal):\n",
      "  Config name: fusion_concat\n",
      "  Dataset: cov_vaccine_degradation\n",
      "  Max Len (filter): 1000\n",
      "  Fusion: concat\n",
      "  Batch size: 32\n",
      "  Epochs: 500\n",
      "  Device: cpu\n",
      "============================================================\n",
      "\n",
      "Loading data...\n",
      "\n",
      "Initializing model...\n",
      "Total number of parameters: 2497753\n",
      "Trainable parameters: 2497753\n",
      "Non-trainable parameters: 0\n",
      "\n",
      "Initializing trainer...\n",
      "\n",
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "Starting training for 500 epochs...\n",
      "Device: cpu\n",
      "Save directory: plots/fusion_concat/cov_vaccine_degradation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:10<00:00,  4.97it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  Train Loss: 1.1657, MSE: 1.1657, Spearman: 0.2603, LR: 0.000030\n",
      "  Val Loss: 0.4941, MSE: 0.4941, Spearman: 0.6864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:09<00:00,  5.47it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500\n",
      "  Train Loss: 1.0804, MSE: 1.0804, Spearman: 0.3997, LR: 0.000030\n",
      "  Val Loss: 0.4164, MSE: 0.4164, Spearman: 0.7122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:08<00:00,  6.24it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500\n",
      "  Train Loss: 0.9973, MSE: 0.9973, Spearman: 0.5152, LR: 0.000030\n",
      "  Val Loss: 0.3611, MSE: 0.3611, Spearman: 0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:09<00:00,  5.36it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500\n",
      "  Train Loss: 0.9349, MSE: 0.9349, Spearman: 0.5528, LR: 0.000030\n",
      "  Val Loss: 0.3172, MSE: 0.3172, Spearman: 0.7520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:11<00:00,  4.31it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 10.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500\n",
      "  Train Loss: 0.8856, MSE: 0.8856, Spearman: 0.5803, LR: 0.000030\n",
      "  Val Loss: 0.2973, MSE: 0.2973, Spearman: 0.7541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:11<00:00,  4.30it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/500\n",
      "  Train Loss: 0.8257, MSE: 0.8257, Spearman: 0.6307, LR: 0.000030\n",
      "  Val Loss: 0.2988, MSE: 0.2988, Spearman: 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:12<00:00,  4.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/500\n",
      "  Train Loss: 0.8009, MSE: 0.8009, Spearman: 0.6322, LR: 0.000030\n",
      "  Val Loss: 0.2406, MSE: 0.2406, Spearman: 0.7756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:13<00:00,  3.76it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/500\n",
      "  Train Loss: 0.7565, MSE: 0.7565, Spearman: 0.6698, LR: 0.000030\n",
      "  Val Loss: 0.2558, MSE: 0.2558, Spearman: 0.7767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:16<00:00,  2.96it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/500\n",
      "  Train Loss: 0.7439, MSE: 0.7439, Spearman: 0.6725, LR: 0.000030\n",
      "  Val Loss: 0.2391, MSE: 0.2391, Spearman: 0.7781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:10<00:00,  4.71it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500\n",
      "  Train Loss: 0.7225, MSE: 0.7225, Spearman: 0.6911, LR: 0.000030\n",
      "  Val Loss: 0.2673, MSE: 0.2673, Spearman: 0.7855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:12<00:00,  3.97it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/500\n",
      "  Train Loss: 0.6991, MSE: 0.6991, Spearman: 0.6948, LR: 0.000030\n",
      "  Val Loss: 0.2204, MSE: 0.2204, Spearman: 0.7928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:12<00:00,  4.14it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 11.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/500\n",
      "  Train Loss: 0.6925, MSE: 0.6925, Spearman: 0.6949, LR: 0.000030\n",
      "  Val Loss: 0.2567, MSE: 0.2567, Spearman: 0.7950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:11<00:00,  4.17it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 10.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/500\n",
      "  Train Loss: 0.6662, MSE: 0.6662, Spearman: 0.7138, LR: 0.000030\n",
      "  Val Loss: 0.2370, MSE: 0.2370, Spearman: 0.7904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:11<00:00,  4.24it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/500\n",
      "  Train Loss: 0.6492, MSE: 0.6492, Spearman: 0.7245, LR: 0.000030\n",
      "  Val Loss: 0.2133, MSE: 0.2133, Spearman: 0.7912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:11<00:00,  4.20it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/500\n",
      "  Train Loss: 0.6325, MSE: 0.6325, Spearman: 0.7363, LR: 0.000030\n",
      "  Val Loss: 0.2109, MSE: 0.2109, Spearman: 0.7949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:13<00:00,  3.73it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/500\n",
      "  Train Loss: 0.6146, MSE: 0.6146, Spearman: 0.7503, LR: 0.000030\n",
      "  Val Loss: 0.2237, MSE: 0.2237, Spearman: 0.7959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:11<00:00,  4.32it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/500\n",
      "  Train Loss: 0.5959, MSE: 0.5959, Spearman: 0.7527, LR: 0.000030\n",
      "  Val Loss: 0.2121, MSE: 0.2121, Spearman: 0.8019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:11<00:00,  4.29it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500\n",
      "  Train Loss: 0.5861, MSE: 0.5861, Spearman: 0.7604, LR: 0.000030\n",
      "  Val Loss: 0.2053, MSE: 0.2053, Spearman: 0.8033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:11<00:00,  4.40it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 11.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/500\n",
      "  Train Loss: 0.5920, MSE: 0.5920, Spearman: 0.7557, LR: 0.000030\n",
      "  Val Loss: 0.2172, MSE: 0.2172, Spearman: 0.8072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:10<00:00,  4.61it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500\n",
      "  Train Loss: 0.5595, MSE: 0.5595, Spearman: 0.7759, LR: 0.000030\n",
      "  Val Loss: 0.2243, MSE: 0.2243, Spearman: 0.7967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:09<00:00,  5.23it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 13.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/500\n",
      "  Train Loss: 0.5346, MSE: 0.5346, Spearman: 0.7820, LR: 0.000030\n",
      "  Val Loss: 0.2203, MSE: 0.2203, Spearman: 0.8093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:08<00:00,  5.92it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 13.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "  Train Loss: 0.5143, MSE: 0.5143, Spearman: 0.7931, LR: 0.000030\n",
      "  Val Loss: 0.1960, MSE: 0.1960, Spearman: 0.8116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.32it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 13.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "  Train Loss: 0.5051, MSE: 0.5051, Spearman: 0.7941, LR: 0.000030\n",
      "  Val Loss: 0.1970, MSE: 0.1970, Spearman: 0.8148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.33it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 16.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "  Train Loss: 0.4984, MSE: 0.4984, Spearman: 0.7966, LR: 0.000030\n",
      "  Val Loss: 0.2067, MSE: 0.2067, Spearman: 0.8028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.50it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 17.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "  Train Loss: 0.4749, MSE: 0.4749, Spearman: 0.8073, LR: 0.000030\n",
      "  Val Loss: 0.2198, MSE: 0.2198, Spearman: 0.8079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.43it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "  Train Loss: 0.4672, MSE: 0.4672, Spearman: 0.8090, LR: 0.000030\n",
      "  Val Loss: 0.2061, MSE: 0.2061, Spearman: 0.8098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.50it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "  Train Loss: 0.4480, MSE: 0.4480, Spearman: 0.8156, LR: 0.000030\n",
      "  Val Loss: 0.2066, MSE: 0.2066, Spearman: 0.8115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.41it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 16.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "  Train Loss: 0.4499, MSE: 0.4499, Spearman: 0.8166, LR: 0.000030\n",
      "  Val Loss: 0.1942, MSE: 0.1942, Spearman: 0.8181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.54it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 16.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "  Train Loss: 0.4487, MSE: 0.4487, Spearman: 0.8143, LR: 0.000030\n",
      "  Val Loss: 0.2041, MSE: 0.2041, Spearman: 0.8066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.64it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "  Train Loss: 0.4190, MSE: 0.4190, Spearman: 0.8296, LR: 0.000030\n",
      "  Val Loss: 0.2275, MSE: 0.2275, Spearman: 0.8155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.76it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 17.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/500\n",
      "  Train Loss: 0.4252, MSE: 0.4252, Spearman: 0.8200, LR: 0.000030\n",
      "  Val Loss: 0.2006, MSE: 0.2006, Spearman: 0.8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  7.12it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/500\n",
      "  Train Loss: 0.4033, MSE: 0.4033, Spearman: 0.8377, LR: 0.000030\n",
      "  Val Loss: 0.2058, MSE: 0.2058, Spearman: 0.8114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.97it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 18.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/500\n",
      "  Train Loss: 0.3726, MSE: 0.3726, Spearman: 0.8465, LR: 0.000030\n",
      "  Val Loss: 0.2108, MSE: 0.2108, Spearman: 0.8115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  7.04it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/500\n",
      "  Train Loss: 0.3644, MSE: 0.3644, Spearman: 0.8447, LR: 0.000030\n",
      "  Val Loss: 0.2040, MSE: 0.2040, Spearman: 0.8152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.97it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/500\n",
      "  Train Loss: 0.3658, MSE: 0.3658, Spearman: 0.8521, LR: 0.000030\n",
      "  Val Loss: 0.1955, MSE: 0.1955, Spearman: 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.94it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 17.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/500\n",
      "  Train Loss: 0.3567, MSE: 0.3567, Spearman: 0.8535, LR: 0.000030\n",
      "  Val Loss: 0.2593, MSE: 0.2593, Spearman: 0.8116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.83it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 17.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/500\n",
      "  Train Loss: 0.3472, MSE: 0.3472, Spearman: 0.8570, LR: 0.000030\n",
      "  Val Loss: 0.2042, MSE: 0.2042, Spearman: 0.8112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  7.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/500\n",
      "  Train Loss: 0.3389, MSE: 0.3389, Spearman: 0.8594, LR: 0.000030\n",
      "  Val Loss: 0.2076, MSE: 0.2076, Spearman: 0.8004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.80it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 18.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/500\n",
      "  Train Loss: 0.3316, MSE: 0.3316, Spearman: 0.8654, LR: 0.000030\n",
      "  Val Loss: 0.1956, MSE: 0.1956, Spearman: 0.8185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.51it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 16.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/500\n",
      "  Train Loss: 0.3121, MSE: 0.3121, Spearman: 0.8770, LR: 0.000030\n",
      "  Val Loss: 0.2167, MSE: 0.2167, Spearman: 0.8120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.54it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 17.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/500\n",
      "  Train Loss: 0.3101, MSE: 0.3101, Spearman: 0.8730, LR: 0.000030\n",
      "  Val Loss: 0.2146, MSE: 0.2146, Spearman: 0.8063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  7.06it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 16.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/500\n",
      "  Train Loss: 0.3182, MSE: 0.3182, Spearman: 0.8714, LR: 0.000030\n",
      "  Val Loss: 0.2517, MSE: 0.2517, Spearman: 0.8027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.56it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 17.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/500\n",
      "  Train Loss: 0.2869, MSE: 0.2869, Spearman: 0.8776, LR: 0.000030\n",
      "  Val Loss: 0.2373, MSE: 0.2373, Spearman: 0.8088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:09<00:00,  5.32it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/500\n",
      "  Train Loss: 0.2846, MSE: 0.2846, Spearman: 0.8836, LR: 0.000030\n",
      "  Val Loss: 0.2138, MSE: 0.2138, Spearman: 0.8082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:06<00:00,  7.67it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/500\n",
      "  Train Loss: 0.2767, MSE: 0.2767, Spearman: 0.8794, LR: 0.000030\n",
      "  Val Loss: 0.2229, MSE: 0.2229, Spearman: 0.8106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:09<00:00,  5.38it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/500\n",
      "  Train Loss: 0.2764, MSE: 0.2764, Spearman: 0.8848, LR: 0.000030\n",
      "  Val Loss: 0.2053, MSE: 0.2053, Spearman: 0.8052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:05<00:00,  8.40it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 21.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/500\n",
      "  Train Loss: 0.2677, MSE: 0.2677, Spearman: 0.8838, LR: 0.000030\n",
      "  Val Loss: 0.2325, MSE: 0.2325, Spearman: 0.8046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:05<00:00,  8.52it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 20.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "  Train Loss: 0.2589, MSE: 0.2589, Spearman: 0.8972, LR: 0.000030\n",
      "  Val Loss: 0.2472, MSE: 0.2472, Spearman: 0.8022\n",
      "\n",
      "Early stopping triggered after 48 epochs!\n",
      "Best validation loss: 0.1942\n",
      "\n",
      "Training completed! Best validation loss: 0.1942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 19.87it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "  Loss: 0.2283\n",
      "  MSE: 0.2283\n",
      "  Spearman: 0.7933\n",
      "\n",
      "============================================================\n",
      "Training completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from data.dataloaders import get_multimodal_loaders\n",
    "from utils.load_config import load_config\n",
    "from utils.calculate_embeddings import calculate_embeddings\n",
    "from models.multimodel import build_model\n",
    "from trainer import RegressionTrainer\n",
    "\n",
    "\n",
    "def _has_any_embeddings(emb_dir) -> bool:\n",
    "    if not os.path.isdir(emb_dir):\n",
    "        return False\n",
    "    try:\n",
    "        for fn in os.listdir(emb_dir):\n",
    "            if fn.endswith(\".pt\"):\n",
    "                return True\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "\n",
    "def main(name: str, dataset: str, max_len: int, batch_size: int, epochs: int):\n",
    "    config = load_config(f\"{name}.yml\")\n",
    "    config[\"Dataset\"] = dataset\n",
    "    config[\"device\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    max_len = int(max_len)\n",
    "\n",
    "    filtered_csv = f\"data/datasets/{config['Dataset']}_multimodal_filtered_maxlen{max_len}.csv\"\n",
    "\n",
    "    dna_dir = f\"embeddings/{config['Dataset']}/DNA/maxlen{max_len}\"\n",
    "    rna_dir = f\"embeddings/{config['Dataset']}/RNA/maxlen{max_len}\"\n",
    "    prot_dir = f\"embeddings/{config['Dataset']}/Protein/maxlen{max_len}\"\n",
    "\n",
    "    need_filtered_csv = not os.path.exists(filtered_csv)\n",
    "    need_dna = not _has_any_embeddings(dna_dir)\n",
    "    need_rna = not _has_any_embeddings(rna_dir)\n",
    "    need_prot = not _has_any_embeddings(prot_dir)\n",
    "    need_embeddings = need_dna or need_rna or need_prot\n",
    "\n",
    "    if need_filtered_csv or need_embeddings:\n",
    "        print(\"Embeddings and/or filtered CSV not found, calculating...\")\n",
    "        for modality in (\"DNA\", \"RNA\", \"Protein\"):\n",
    "            calculate_embeddings(\n",
    "                dataset=config[\"Dataset\"],\n",
    "                modality=modality,\n",
    "                device=config[\"device\"],\n",
    "                max_len=max_len,\n",
    "            )\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training Configuration (Multimodal):\")\n",
    "    print(f\"  Config name: {config.get('name', name)}\")\n",
    "    print(f\"  Dataset: {config['Dataset']}\")\n",
    "    print(f\"  Max Len (filter): {max_len}\")\n",
    "    print(f\"  Fusion: {config.get('fusion_type', 'concat')}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Device: {config['device']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nLoading data...\")\n",
    "    train_loader, val_loader, test_loader = get_multimodal_loaders(\n",
    "        config[\"Dataset\"],\n",
    "        batch_size=batch_size,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = build_model(config)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "    print(\"\\nInitializing trainer...\")\n",
    "    trainer = RegressionTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=config[\"device\"],\n",
    "        save_dir=f\"./plots/{config.get('name', name)}/{config['Dataset']}\",\n",
    "    )\n",
    "\n",
    "    # entropy reg for MIL (nested config with backwards-compatible fallback)\n",
    "    lam_entropy = None\n",
    "    if isinstance(config.get(\"trainer\", None), dict):\n",
    "        lam_entropy = config[\"trainer\"].get(\"lam_entropy\", None)\n",
    "    if lam_entropy is None:\n",
    "        lam_entropy = config.get(\"lam_entropy\", None)\n",
    "\n",
    "    if lam_entropy is not None:\n",
    "        trainer.lam_entropy = float(lam_entropy)\n",
    "        if trainer.lam_entropy > 0:\n",
    "            print(f\"Using MIL entropy regularization: lam_entropy={trainer.lam_entropy}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    trainer.train(epochs=epochs)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training completed!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train multimodal model (DNA+RNA+Protein)\")\n",
    "    parser.add_argument(\n",
    "        \"--name\",\n",
    "        type=str,\n",
    "        default=\"fusion_concat\",\n",
    "        help=\"Config file name (without .yml). Options: fusion_concat, fusion_mil, fusion_xattn\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"cov_vaccine_degradation\",\n",
    "        help=(\n",
    "            \"Dataset to use. Default: fungal_expression. Options: \"\n",
    "            \"'mrna_stability', 'ecoli_proteins', 'cov_vaccine_degradation', 'fungal_expression'\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-len\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help=\"Filter threshold: keep only sequences with raw length <= max_len before embedding/training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Batch size.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"Number of training epochs.\",\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    main(\n",
    "        name=args.name,\n",
    "        dataset=args.dataset,\n",
    "        max_len=args.max_len,\n",
    "        batch_size=args.batch_size,\n",
    "        epochs=args.epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2ef2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Configuration (Multimodal):\n",
      "  Config name: fusion_mil\n",
      "  Dataset: cov_vaccine_degradation\n",
      "  Max Len (filter): 1000\n",
      "  Fusion: mil\n",
      "  Batch size: 32\n",
      "  Epochs: 500\n",
      "  Device: cpu\n",
      "============================================================\n",
      "\n",
      "Loading data...\n",
      "\n",
      "Initializing model...\n",
      "Total number of parameters: 3185374\n",
      "Trainable parameters: 3185374\n",
      "Non-trainable parameters: 0\n",
      "\n",
      "Initializing trainer...\n",
      "Using MIL entropy regularization: lam_entropy=0.01\n",
      "\n",
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "Starting training for 500 epochs...\n",
      "Device: cpu\n",
      "Save directory: plots/fusion_mil/cov_vaccine_degradation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:10<00:00,  4.92it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  Train Loss: 1.2053, MSE: 1.1944, Spearman: 0.1818, LR: 0.000030\n",
      "  Val Loss: 0.5678, MSE: 0.5569, Spearman: 0.6082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:08<00:00,  5.96it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500\n",
      "  Train Loss: 1.1636, MSE: 1.1528, Spearman: 0.2765, LR: 0.000030\n",
      "  Val Loss: 0.4996, MSE: 0.4890, Spearman: 0.6371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.39it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 13.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500\n",
      "  Train Loss: 1.0946, MSE: 1.0843, Spearman: 0.4443, LR: 0.000030\n",
      "  Val Loss: 0.4220, MSE: 0.4120, Spearman: 0.6461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.35it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500\n",
      "  Train Loss: 1.0093, MSE: 0.9997, Spearman: 0.4852, LR: 0.000030\n",
      "  Val Loss: 0.3441, MSE: 0.3351, Spearman: 0.6638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.62it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500\n",
      "  Train Loss: 0.9348, MSE: 0.9260, Spearman: 0.5277, LR: 0.000030\n",
      "  Val Loss: 0.3139, MSE: 0.3053, Spearman: 0.6751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:08<00:00,  6.00it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/500\n",
      "  Train Loss: 0.8758, MSE: 0.8674, Spearman: 0.5793, LR: 0.000030\n",
      "  Val Loss: 0.3239, MSE: 0.3157, Spearman: 0.6838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.79it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/500\n",
      "  Train Loss: 0.8452, MSE: 0.8372, Spearman: 0.6000, LR: 0.000030\n",
      "  Val Loss: 0.2886, MSE: 0.2808, Spearman: 0.6907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:08<00:00,  6.23it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/500\n",
      "  Train Loss: 0.8215, MSE: 0.8138, Spearman: 0.6221, LR: 0.000030\n",
      "  Val Loss: 0.2879, MSE: 0.2804, Spearman: 0.7008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.34it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/500\n",
      "  Train Loss: 0.7716, MSE: 0.7642, Spearman: 0.6563, LR: 0.000030\n",
      "  Val Loss: 0.2729, MSE: 0.2654, Spearman: 0.7092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.35it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500\n",
      "  Train Loss: 0.7489, MSE: 0.7417, Spearman: 0.6681, LR: 0.000030\n",
      "  Val Loss: 0.2807, MSE: 0.2739, Spearman: 0.7095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.29it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/500\n",
      "  Train Loss: 0.7288, MSE: 0.7220, Spearman: 0.6791, LR: 0.000030\n",
      "  Val Loss: 0.2706, MSE: 0.2638, Spearman: 0.7132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.41it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/500\n",
      "  Train Loss: 0.7373, MSE: 0.7306, Spearman: 0.6602, LR: 0.000030\n",
      "  Val Loss: 0.3991, MSE: 0.3923, Spearman: 0.7072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.29it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 18.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/500\n",
      "  Train Loss: 0.6898, MSE: 0.6830, Spearman: 0.7031, LR: 0.000030\n",
      "  Val Loss: 0.2619, MSE: 0.2554, Spearman: 0.7305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.48it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/500\n",
      "  Train Loss: 0.6772, MSE: 0.6709, Spearman: 0.7088, LR: 0.000030\n",
      "  Val Loss: 0.2760, MSE: 0.2698, Spearman: 0.7178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.54it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 17.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/500\n",
      "  Train Loss: 0.6488, MSE: 0.6428, Spearman: 0.7240, LR: 0.000030\n",
      "  Val Loss: 0.2865, MSE: 0.2805, Spearman: 0.7173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:08<00:00,  6.15it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/500\n",
      "  Train Loss: 0.6141, MSE: 0.6083, Spearman: 0.7369, LR: 0.000030\n",
      "  Val Loss: 0.3139, MSE: 0.3084, Spearman: 0.7067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.72it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/500\n",
      "  Train Loss: 0.5996, MSE: 0.5942, Spearman: 0.7462, LR: 0.000030\n",
      "  Val Loss: 0.2874, MSE: 0.2821, Spearman: 0.7307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.38it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500\n",
      "  Train Loss: 0.5861, MSE: 0.5808, Spearman: 0.7522, LR: 0.000030\n",
      "  Val Loss: 0.3563, MSE: 0.3517, Spearman: 0.7083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:08<00:00,  6.13it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/500\n",
      "  Train Loss: 0.5680, MSE: 0.5631, Spearman: 0.7589, LR: 0.000030\n",
      "  Val Loss: 0.2976, MSE: 0.2928, Spearman: 0.7045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.27it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 14.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500\n",
      "  Train Loss: 0.5358, MSE: 0.5310, Spearman: 0.7707, LR: 0.000030\n",
      "  Val Loss: 0.3156, MSE: 0.3111, Spearman: 0.7098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.38it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/500\n",
      "  Train Loss: 0.5106, MSE: 0.5061, Spearman: 0.7889, LR: 0.000030\n",
      "  Val Loss: 0.3208, MSE: 0.3165, Spearman: 0.7123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:08<00:00,  5.69it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "  Train Loss: 0.4955, MSE: 0.4912, Spearman: 0.7902, LR: 0.000030\n",
      "  Val Loss: 0.3026, MSE: 0.2985, Spearman: 0.7087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:08<00:00,  5.85it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 14.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "  Train Loss: 0.4856, MSE: 0.4816, Spearman: 0.7900, LR: 0.000030\n",
      "  Val Loss: 0.3348, MSE: 0.3309, Spearman: 0.6931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.26it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "  Train Loss: 0.4617, MSE: 0.4579, Spearman: 0.7952, LR: 0.000030\n",
      "  Val Loss: 0.3694, MSE: 0.3656, Spearman: 0.6948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.35it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "  Train Loss: 0.4526, MSE: 0.4487, Spearman: 0.8006, LR: 0.000030\n",
      "  Val Loss: 0.3357, MSE: 0.3319, Spearman: 0.6967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.33it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "  Train Loss: 0.4154, MSE: 0.4118, Spearman: 0.8194, LR: 0.000030\n",
      "  Val Loss: 0.3506, MSE: 0.3473, Spearman: 0.6898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.37it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "  Train Loss: 0.4174, MSE: 0.4141, Spearman: 0.8240, LR: 0.000030\n",
      "  Val Loss: 0.3467, MSE: 0.3434, Spearman: 0.6861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.42it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "  Train Loss: 0.3789, MSE: 0.3756, Spearman: 0.8359, LR: 0.000030\n",
      "  Val Loss: 0.3740, MSE: 0.3709, Spearman: 0.6783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.39it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "  Train Loss: 0.3657, MSE: 0.3627, Spearman: 0.8333, LR: 0.000030\n",
      "  Val Loss: 0.3574, MSE: 0.3545, Spearman: 0.6808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:08<00:00,  5.96it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 18.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "  Train Loss: 0.3535, MSE: 0.3506, Spearman: 0.8431, LR: 0.000030\n",
      "  Val Loss: 0.3708, MSE: 0.3679, Spearman: 0.6815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:08<00:00,  6.15it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 18.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/500\n",
      "  Train Loss: 0.3317, MSE: 0.3289, Spearman: 0.8519, LR: 0.000030\n",
      "  Val Loss: 0.3931, MSE: 0.3904, Spearman: 0.6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.52it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/500\n",
      "  Train Loss: 0.3168, MSE: 0.3142, Spearman: 0.8548, LR: 0.000030\n",
      "  Val Loss: 0.3827, MSE: 0.3802, Spearman: 0.6590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:07<00:00,  6.49it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 13.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/500\n",
      "  Train Loss: 0.3075, MSE: 0.3051, Spearman: 0.8625, LR: 0.000030\n",
      "  Val Loss: 0.3672, MSE: 0.3647, Spearman: 0.6751\n",
      "\n",
      "Early stopping triggered after 33 epochs!\n",
      "Best validation loss: 0.2619\n",
      "\n",
      "Training completed! Best validation loss: 0.2619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 15.12it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:01<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "  Loss: 0.3121\n",
      "  MSE: 0.3055\n",
      "  Spearman: 0.7007\n",
      "\n",
      "============================================================\n",
      "Training completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from data.dataloaders import get_multimodal_loaders\n",
    "from utils.load_config import load_config\n",
    "from utils.calculate_embeddings import calculate_embeddings\n",
    "from models.multimodel import build_model\n",
    "from trainer import RegressionTrainer\n",
    "\n",
    "\n",
    "def _has_any_embeddings(emb_dir) -> bool:\n",
    "    if not os.path.isdir(emb_dir):\n",
    "        return False\n",
    "    try:\n",
    "        for fn in os.listdir(emb_dir):\n",
    "            if fn.endswith(\".pt\"):\n",
    "                return True\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "\n",
    "def main(name: str, dataset: str, max_len: int, batch_size: int, epochs: int):\n",
    "    config = load_config(f\"{name}.yml\")\n",
    "    config[\"Dataset\"] = dataset\n",
    "    config[\"device\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    max_len = int(max_len)\n",
    "\n",
    "    filtered_csv = f\"data/datasets/{config['Dataset']}_multimodal_filtered_maxlen{max_len}.csv\"\n",
    "\n",
    "    dna_dir = f\"embeddings/{config['Dataset']}/DNA/maxlen{max_len}\"\n",
    "    rna_dir = f\"embeddings/{config['Dataset']}/RNA/maxlen{max_len}\"\n",
    "    prot_dir = f\"embeddings/{config['Dataset']}/Protein/maxlen{max_len}\"\n",
    "\n",
    "    need_filtered_csv = not os.path.exists(filtered_csv)\n",
    "    need_dna = not _has_any_embeddings(dna_dir)\n",
    "    need_rna = not _has_any_embeddings(rna_dir)\n",
    "    need_prot = not _has_any_embeddings(prot_dir)\n",
    "    need_embeddings = need_dna or need_rna or need_prot\n",
    "\n",
    "    if need_filtered_csv or need_embeddings:\n",
    "        print(\"Embeddings and/or filtered CSV not found, calculating...\")\n",
    "        for modality in (\"DNA\", \"RNA\", \"Protein\"):\n",
    "            calculate_embeddings(\n",
    "                dataset=config[\"Dataset\"],\n",
    "                modality=modality,\n",
    "                device=config[\"device\"],\n",
    "                max_len=max_len,\n",
    "            )\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training Configuration (Multimodal):\")\n",
    "    print(f\"  Config name: {config.get('name', name)}\")\n",
    "    print(f\"  Dataset: {config['Dataset']}\")\n",
    "    print(f\"  Max Len (filter): {max_len}\")\n",
    "    print(f\"  Fusion: {config.get('fusion_type', 'concat')}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Device: {config['device']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nLoading data...\")\n",
    "    train_loader, val_loader, test_loader = get_multimodal_loaders(\n",
    "        config[\"Dataset\"],\n",
    "        batch_size=batch_size,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = build_model(config)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "    print(\"\\nInitializing trainer...\")\n",
    "    trainer = RegressionTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=config[\"device\"],\n",
    "        save_dir=f\"./plots/{config.get('name', name)}/{config['Dataset']}\",\n",
    "    )\n",
    "\n",
    "    # entropy reg for MIL (nested config with backwards-compatible fallback)\n",
    "    lam_entropy = None\n",
    "    if isinstance(config.get(\"trainer\", None), dict):\n",
    "        lam_entropy = config[\"trainer\"].get(\"lam_entropy\", None)\n",
    "    if lam_entropy is None:\n",
    "        lam_entropy = config.get(\"lam_entropy\", None)\n",
    "\n",
    "    if lam_entropy is not None:\n",
    "        trainer.lam_entropy = float(lam_entropy)\n",
    "        if trainer.lam_entropy > 0:\n",
    "            print(f\"Using MIL entropy regularization: lam_entropy={trainer.lam_entropy}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    trainer.train(epochs=epochs)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training completed!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train multimodal model (DNA+RNA+Protein)\")\n",
    "    parser.add_argument(\n",
    "        \"--name\",\n",
    "        type=str,\n",
    "        default=\"fusion_mil\",\n",
    "        help=\"Config file name (without .yml). Options: fusion_concat, fusion_mil, fusion_xattn\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"cov_vaccine_degradation\",\n",
    "        help=(\n",
    "            \"Dataset to use. Default: fungal_expression. Options: \"\n",
    "            \"'mrna_stability', 'ecoli_proteins', 'cov_vaccine_degradation', 'fungal_expression'\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-len\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help=\"Filter threshold: keep only sequences with raw length <= max_len before embedding/training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Batch size.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"Number of training epochs.\",\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    main(\n",
    "        name=args.name,\n",
    "        dataset=args.dataset,\n",
    "        max_len=args.max_len,\n",
    "        batch_size=args.batch_size,\n",
    "        epochs=args.epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d6d099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Configuration (Multimodal):\n",
      "  Config name: fusion_xattn\n",
      "  Dataset: cov_vaccine_degradation\n",
      "  Max Len (filter): 1000\n",
      "  Fusion: xattn\n",
      "  Batch size: 32\n",
      "  Epochs: 500\n",
      "  Device: cpu\n",
      "============================================================\n",
      "\n",
      "Loading data...\n",
      "\n",
      "Initializing model...\n",
      "Total number of parameters: 9315473\n",
      "Trainable parameters: 9315473\n",
      "Non-trainable parameters: 0\n",
      "\n",
      "Initializing trainer...\n",
      "\n",
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "Starting training for 500 epochs...\n",
      "Device: cpu\n",
      "Save directory: plots/fusion_xattn/cov_vaccine_degradation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:26<00:00,  1.86it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  Train Loss: 1.2714, MSE: 1.2714, Spearman: 0.0256, LR: 0.000030\n",
      "  Val Loss: 0.5419, MSE: 0.5419, Spearman: 0.5023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:30<00:00,  1.62it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500\n",
      "  Train Loss: 1.1629, MSE: 1.1629, Spearman: 0.2077, LR: 0.000030\n",
      "  Val Loss: 0.4848, MSE: 0.4848, Spearman: 0.5456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:32<00:00,  1.53it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500\n",
      "  Train Loss: 1.1197, MSE: 1.1197, Spearman: 0.2856, LR: 0.000030\n",
      "  Val Loss: 0.4641, MSE: 0.4641, Spearman: 0.5780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:33<00:00,  1.51it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:03<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500\n",
      "  Train Loss: 1.0659, MSE: 1.0659, Spearman: 0.3484, LR: 0.000030\n",
      "  Val Loss: 0.4321, MSE: 0.4321, Spearman: 0.5884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:33<00:00,  1.50it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:03<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500\n",
      "  Train Loss: 0.9936, MSE: 0.9936, Spearman: 0.4462, LR: 0.000030\n",
      "  Val Loss: 0.4379, MSE: 0.4379, Spearman: 0.6336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:32<00:00,  1.56it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:03<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/500\n",
      "  Train Loss: 1.0044, MSE: 1.0044, Spearman: 0.4297, LR: 0.000030\n",
      "  Val Loss: 0.3981, MSE: 0.3981, Spearman: 0.6477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:38<00:00,  1.30it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/500\n",
      "  Train Loss: 0.9165, MSE: 0.9165, Spearman: 0.5104, LR: 0.000030\n",
      "  Val Loss: 0.3625, MSE: 0.3625, Spearman: 0.6784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:33<00:00,  1.51it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/500\n",
      "  Train Loss: 0.8990, MSE: 0.8990, Spearman: 0.5454, LR: 0.000030\n",
      "  Val Loss: 0.3538, MSE: 0.3538, Spearman: 0.7130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:30<00:00,  1.65it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/500\n",
      "  Train Loss: 0.8875, MSE: 0.8875, Spearman: 0.5503, LR: 0.000030\n",
      "  Val Loss: 0.3274, MSE: 0.3274, Spearman: 0.6928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:30<00:00,  1.62it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500\n",
      "  Train Loss: 0.8793, MSE: 0.8793, Spearman: 0.5215, LR: 0.000030\n",
      "  Val Loss: 0.3054, MSE: 0.3054, Spearman: 0.7275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:29<00:00,  1.71it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/500\n",
      "  Train Loss: 0.8421, MSE: 0.8421, Spearman: 0.5857, LR: 0.000030\n",
      "  Val Loss: 0.4045, MSE: 0.4045, Spearman: 0.7174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:28<00:00,  1.76it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/500\n",
      "  Train Loss: 0.8146, MSE: 0.8146, Spearman: 0.5984, LR: 0.000030\n",
      "  Val Loss: 0.3477, MSE: 0.3477, Spearman: 0.7345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.83it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/500\n",
      "  Train Loss: 0.7971, MSE: 0.7971, Spearman: 0.5983, LR: 0.000030\n",
      "  Val Loss: 0.3774, MSE: 0.3774, Spearman: 0.7440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:25<00:00,  1.94it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/500\n",
      "  Train Loss: 0.7690, MSE: 0.7690, Spearman: 0.5953, LR: 0.000030\n",
      "  Val Loss: 0.3977, MSE: 0.3977, Spearman: 0.7057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:25<00:00,  1.92it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/500\n",
      "  Train Loss: 0.8064, MSE: 0.8064, Spearman: 0.5799, LR: 0.000030\n",
      "  Val Loss: 0.3469, MSE: 0.3469, Spearman: 0.7519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:25<00:00,  1.95it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/500\n",
      "  Train Loss: 0.7409, MSE: 0.7409, Spearman: 0.6291, LR: 0.000030\n",
      "  Val Loss: 0.3348, MSE: 0.3348, Spearman: 0.7444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:25<00:00,  1.95it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/500\n",
      "  Train Loss: 0.7194, MSE: 0.7194, Spearman: 0.6231, LR: 0.000030\n",
      "  Val Loss: 0.3489, MSE: 0.3489, Spearman: 0.7720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:25<00:00,  1.95it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500\n",
      "  Train Loss: 0.6879, MSE: 0.6879, Spearman: 0.6505, LR: 0.000030\n",
      "  Val Loss: 0.2785, MSE: 0.2785, Spearman: 0.7668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:28<00:00,  1.75it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/500\n",
      "  Train Loss: 0.6594, MSE: 0.6594, Spearman: 0.6438, LR: 0.000030\n",
      "  Val Loss: 0.2958, MSE: 0.2958, Spearman: 0.7657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:26<00:00,  1.86it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500\n",
      "  Train Loss: 0.6580, MSE: 0.6580, Spearman: 0.6544, LR: 0.000030\n",
      "  Val Loss: 0.3336, MSE: 0.3336, Spearman: 0.7377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.83it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/500\n",
      "  Train Loss: 0.6479, MSE: 0.6479, Spearman: 0.6577, LR: 0.000030\n",
      "  Val Loss: 0.3093, MSE: 0.3093, Spearman: 0.7372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.84it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500\n",
      "  Train Loss: 0.6046, MSE: 0.6046, Spearman: 0.6713, LR: 0.000030\n",
      "  Val Loss: 0.3275, MSE: 0.3275, Spearman: 0.7397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.84it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500\n",
      "  Train Loss: 0.5866, MSE: 0.5866, Spearman: 0.6627, LR: 0.000030\n",
      "  Val Loss: 0.3604, MSE: 0.3604, Spearman: 0.7126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.85it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "  Train Loss: 0.6617, MSE: 0.6617, Spearman: 0.6255, LR: 0.000030\n",
      "  Val Loss: 0.3151, MSE: 0.3151, Spearman: 0.7633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.85it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "  Train Loss: 0.6092, MSE: 0.6092, Spearman: 0.6751, LR: 0.000030\n",
      "  Val Loss: 0.3216, MSE: 0.3216, Spearman: 0.7432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.85it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500\n",
      "  Train Loss: 0.6374, MSE: 0.6374, Spearman: 0.6303, LR: 0.000030\n",
      "  Val Loss: 0.3556, MSE: 0.3556, Spearman: 0.7088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.85it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "  Train Loss: 0.6053, MSE: 0.6053, Spearman: 0.6526, LR: 0.000030\n",
      "  Val Loss: 0.3349, MSE: 0.3349, Spearman: 0.7524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:26<00:00,  1.86it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "  Train Loss: 0.5138, MSE: 0.5138, Spearman: 0.6933, LR: 0.000030\n",
      "  Val Loss: 0.2866, MSE: 0.2866, Spearman: 0.7653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:26<00:00,  1.86it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "  Train Loss: 0.5280, MSE: 0.5280, Spearman: 0.7057, LR: 0.000030\n",
      "  Val Loss: 0.3214, MSE: 0.3214, Spearman: 0.7570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:26<00:00,  1.85it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "  Train Loss: 0.5379, MSE: 0.5379, Spearman: 0.6817, LR: 0.000030\n",
      "  Val Loss: 0.3162, MSE: 0.3162, Spearman: 0.7336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:30<00:00,  1.67it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/500\n",
      "  Train Loss: 0.5243, MSE: 0.5243, Spearman: 0.7038, LR: 0.000030\n",
      "  Val Loss: 0.3158, MSE: 0.3158, Spearman: 0.7483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.83it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/500\n",
      "  Train Loss: 0.4812, MSE: 0.4812, Spearman: 0.7101, LR: 0.000030\n",
      "  Val Loss: 0.3183, MSE: 0.3183, Spearman: 0.7561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.83it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/500\n",
      "  Train Loss: 0.5028, MSE: 0.5028, Spearman: 0.7162, LR: 0.000030\n",
      "  Val Loss: 0.3198, MSE: 0.3198, Spearman: 0.7365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.83it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/500\n",
      "  Train Loss: 0.5158, MSE: 0.5158, Spearman: 0.7060, LR: 0.000030\n",
      "  Val Loss: 0.2982, MSE: 0.2982, Spearman: 0.7744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.79it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/500\n",
      "  Train Loss: 0.4750, MSE: 0.4750, Spearman: 0.7290, LR: 0.000030\n",
      "  Val Loss: 0.3271, MSE: 0.3271, Spearman: 0.7111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.84it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/500\n",
      "  Train Loss: 0.4534, MSE: 0.4534, Spearman: 0.7342, LR: 0.000030\n",
      "  Val Loss: 0.3666, MSE: 0.3666, Spearman: 0.7587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.82it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/500\n",
      "  Train Loss: 0.5716, MSE: 0.5716, Spearman: 0.6922, LR: 0.000030\n",
      "  Val Loss: 0.2879, MSE: 0.2879, Spearman: 0.7518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████| 50/50 [00:27<00:00,  1.81it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/500\n",
      "  Train Loss: 0.4182, MSE: 0.4182, Spearman: 0.7463, LR: 0.000030\n",
      "  Val Loss: 0.3069, MSE: 0.3069, Spearman: 0.7615\n",
      "\n",
      "Early stopping triggered after 38 epochs!\n",
      "Best validation loss: 0.2785\n",
      "\n",
      "Training completed! Best validation loss: 0.2785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.85it/s]\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:02<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "  Loss: 0.3011\n",
      "  MSE: 0.3011\n",
      "  Spearman: 0.7285\n",
      "\n",
      "============================================================\n",
      "Training completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from data.dataloaders import get_multimodal_loaders\n",
    "from utils.load_config import load_config\n",
    "from utils.calculate_embeddings import calculate_embeddings\n",
    "from models.multimodel import build_model\n",
    "from trainer import RegressionTrainer\n",
    "\n",
    "\n",
    "def _has_any_embeddings(emb_dir) -> bool:\n",
    "    if not os.path.isdir(emb_dir):\n",
    "        return False\n",
    "    try:\n",
    "        for fn in os.listdir(emb_dir):\n",
    "            if fn.endswith(\".pt\"):\n",
    "                return True\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "\n",
    "def main(name: str, dataset: str, max_len: int, batch_size: int, epochs: int):\n",
    "    config = load_config(f\"{name}.yml\")\n",
    "    config[\"Dataset\"] = dataset\n",
    "    config[\"device\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    max_len = int(max_len)\n",
    "\n",
    "    filtered_csv = f\"data/datasets/{config['Dataset']}_multimodal_filtered_maxlen{max_len}.csv\"\n",
    "\n",
    "    dna_dir = f\"embeddings/{config['Dataset']}/DNA/maxlen{max_len}\"\n",
    "    rna_dir = f\"embeddings/{config['Dataset']}/RNA/maxlen{max_len}\"\n",
    "    prot_dir = f\"embeddings/{config['Dataset']}/Protein/maxlen{max_len}\"\n",
    "\n",
    "    need_filtered_csv = not os.path.exists(filtered_csv)\n",
    "    need_dna = not _has_any_embeddings(dna_dir)\n",
    "    need_rna = not _has_any_embeddings(rna_dir)\n",
    "    need_prot = not _has_any_embeddings(prot_dir)\n",
    "    need_embeddings = need_dna or need_rna or need_prot\n",
    "\n",
    "    if need_filtered_csv or need_embeddings:\n",
    "        print(\"Embeddings and/or filtered CSV not found, calculating...\")\n",
    "        for modality in (\"DNA\", \"RNA\", \"Protein\"):\n",
    "            calculate_embeddings(\n",
    "                dataset=config[\"Dataset\"],\n",
    "                modality=modality,\n",
    "                device=config[\"device\"],\n",
    "                max_len=max_len,\n",
    "            )\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training Configuration (Multimodal):\")\n",
    "    print(f\"  Config name: {config.get('name', name)}\")\n",
    "    print(f\"  Dataset: {config['Dataset']}\")\n",
    "    print(f\"  Max Len (filter): {max_len}\")\n",
    "    print(f\"  Fusion: {config.get('fusion_type', 'concat')}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Device: {config['device']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nLoading data...\")\n",
    "    train_loader, val_loader, test_loader = get_multimodal_loaders(\n",
    "        config[\"Dataset\"],\n",
    "        batch_size=batch_size,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = build_model(config)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "    print(\"\\nInitializing trainer...\")\n",
    "    trainer = RegressionTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=config[\"device\"],\n",
    "        save_dir=f\"./plots/{config.get('name', name)}/{config['Dataset']}\",\n",
    "    )\n",
    "\n",
    "    # entropy reg for MIL (nested config with backwards-compatible fallback)\n",
    "    lam_entropy = None\n",
    "    if isinstance(config.get(\"trainer\", None), dict):\n",
    "        lam_entropy = config[\"trainer\"].get(\"lam_entropy\", None)\n",
    "    if lam_entropy is None:\n",
    "        lam_entropy = config.get(\"lam_entropy\", None)\n",
    "\n",
    "    if lam_entropy is not None:\n",
    "        trainer.lam_entropy = float(lam_entropy)\n",
    "        if trainer.lam_entropy > 0:\n",
    "            print(f\"Using MIL entropy regularization: lam_entropy={trainer.lam_entropy}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    trainer.train(epochs=epochs)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training completed!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train multimodal model (DNA+RNA+Protein)\")\n",
    "    parser.add_argument(\n",
    "        \"--name\",\n",
    "        type=str,\n",
    "        default=\"fusion_xattn\",\n",
    "        help=\"Config file name (without .yml). Options: fusion_concat, fusion_mil, fusion_xattn\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"cov_vaccine_degradation\",\n",
    "        help=(\n",
    "            \"Dataset to use. Default: fungal_expression. Options: \"\n",
    "            \"'mrna_stability', 'ecoli_proteins', 'cov_vaccine_degradation', 'fungal_expression'\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-len\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help=\"Filter threshold: keep only sequences with raw length <= max_len before embedding/training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Batch size.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"Number of training epochs.\",\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    main(\n",
    "        name=args.name,\n",
    "        dataset=args.dataset,\n",
    "        max_len=args.max_len,\n",
    "        batch_size=args.batch_size,\n",
    "        epochs=args.epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9723551",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1aafd",
   "metadata": {},
   "source": [
    "# ecoli_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e9714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings and/or filtered CSV not found, calculating...\n",
      "Calculating embeddings... for dataset: ecoli_proteins and modality: DNA\n",
      "============================================================\n",
      "Filtering rule for CSV: RNA length <= 1000\n",
      "============================================================\n",
      "Created filtered CSV (RNA-length based).\n",
      "Original rows : 6348\n",
      "Kept rows     : 4450 (RNA length <= 1000)\n",
      "Saved to      : data/datasets/ecoli_proteins_multimodal_filtered_maxlen1000.csv\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sahandhassani/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Embedding ecoli_proteins from DNA:   0%|       | 1/4450 [00:00<24:04,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First embedding shape: torch.Size([133, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding ecoli_proteins from DNA:  58%|██▎ | 2589/4450 [10:23<05:33,  5.58it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from data.dataloaders import get_multimodal_loaders\n",
    "from utils.load_config import load_config\n",
    "from utils.calculate_embeddings import calculate_embeddings\n",
    "from models.multimodel import build_model\n",
    "from trainer import RegressionTrainer\n",
    "\n",
    "\n",
    "def _has_any_embeddings(emb_dir) -> bool:\n",
    "    if not os.path.isdir(emb_dir):\n",
    "        return False\n",
    "    try:\n",
    "        for fn in os.listdir(emb_dir):\n",
    "            if fn.endswith(\".pt\"):\n",
    "                return True\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def main(name: str, dataset: str, max_len: int, batch_size: int, epochs: int):\n",
    "    config = load_config(f\"{name}.yml\")\n",
    "    config[\"Dataset\"] = dataset\n",
    "    config[\"device\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    max_len = int(max_len)\n",
    "\n",
    "    filtered_csv = f\"data/datasets/{config['Dataset']}_multimodal_filtered_maxlen{max_len}.csv\"\n",
    "\n",
    "    dna_dir = f\"embeddings/{config['Dataset']}/DNA/maxlen{max_len}\"\n",
    "    rna_dir = f\"embeddings/{config['Dataset']}/RNA/maxlen{max_len}\"\n",
    "    prot_dir = f\"embeddings/{config['Dataset']}/Protein/maxlen{max_len}\"\n",
    "\n",
    "    need_filtered_csv = not os.path.exists(filtered_csv)\n",
    "    need_dna = not _has_any_embeddings(dna_dir)\n",
    "    need_rna = not _has_any_embeddings(rna_dir)\n",
    "    need_prot = not _has_any_embeddings(prot_dir)\n",
    "    need_embeddings = need_dna or need_rna or need_prot\n",
    "\n",
    "    if need_filtered_csv or need_embeddings:\n",
    "        print(\"Embeddings and/or filtered CSV not found, calculating...\")\n",
    "        for modality in (\"DNA\", \"RNA\", \"Protein\"):\n",
    "            calculate_embeddings(\n",
    "                dataset=config[\"Dataset\"],\n",
    "                modality=modality,\n",
    "                device=config[\"device\"],\n",
    "                max_len=max_len,\n",
    "            )\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training Configuration (Multimodal):\")\n",
    "    print(f\"  Config name: {config.get('name', name)}\")\n",
    "    print(f\"  Dataset: {config['Dataset']}\")\n",
    "    print(f\"  Max Len (filter): {max_len}\")\n",
    "    print(f\"  Fusion: {config.get('fusion_type', 'concat')}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Device: {config['device']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nLoading data...\")\n",
    "    train_loader, val_loader, test_loader = get_multimodal_loaders(\n",
    "        config[\"Dataset\"],\n",
    "        batch_size=batch_size,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = build_model(config)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "    print(\"\\nInitializing trainer...\")\n",
    "    trainer = ClassificationTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=config[\"device\"],\n",
    "        save_dir=f\"./plots/{config.get('name', name)}/{config['Dataset']}\",\n",
    "    )\n",
    "\n",
    "    # entropy reg for MIL (nested config with backwards-compatible fallback)\n",
    "    lam_entropy = None\n",
    "    if isinstance(config.get(\"trainer\", None), dict):\n",
    "        lam_entropy = config[\"trainer\"].get(\"lam_entropy\", None)\n",
    "    if lam_entropy is None:\n",
    "        lam_entropy = config.get(\"lam_entropy\", None)\n",
    "\n",
    "    if lam_entropy is not None:\n",
    "        trainer.lam_entropy = float(lam_entropy)\n",
    "        if trainer.lam_entropy > 0:\n",
    "            print(f\"Using MIL entropy regularization: lam_entropy={trainer.lam_entropy}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    trainer.train(epochs=epochs)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training completed!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train multimodal model (DNA+RNA+Protein)\")\n",
    "    parser.add_argument(\n",
    "        \"--name\",\n",
    "        type=str,\n",
    "        default=\"fusion_concat\",\n",
    "        help=\"Config file name (without .yml). Options: fusion_concat, fusion_mil, fusion_xattn\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"ecoli_proteins\",\n",
    "        help=(\n",
    "            \"Dataset to use. Default: fungal_expression. Options: \"\n",
    "            \"'mrna_stability', 'ecoli_proteins', 'cov_vaccine_degradation', 'fungal_expression'\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-len\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help=\"Filter threshold: keep only sequences with raw length <= max_len before embedding/training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Batch size.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"Number of training epochs.\",\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    main(\n",
    "        name=args.name,\n",
    "        dataset=args.dataset,\n",
    "        max_len=args.max_len,\n",
    "        batch_size=args.batch_size,\n",
    "        epochs=args.epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b23c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from data.dataloaders import get_multimodal_loaders\n",
    "from utils.load_config import load_config\n",
    "from utils.calculate_embeddings import calculate_embeddings\n",
    "from models.multimodel import build_model\n",
    "from trainer import RegressionTrainer\n",
    "\n",
    "\n",
    "def _has_any_embeddings(emb_dir) -> bool:\n",
    "    if not os.path.isdir(emb_dir):\n",
    "        return False\n",
    "    try:\n",
    "        for fn in os.listdir(emb_dir):\n",
    "            if fn.endswith(\".pt\"):\n",
    "                return True\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def main(name: str, dataset: str, max_len: int, batch_size: int, epochs: int):\n",
    "    config = load_config(f\"{name}.yml\")\n",
    "    config[\"Dataset\"] = dataset\n",
    "    config[\"device\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    max_len = int(max_len)\n",
    "\n",
    "    filtered_csv = f\"data/datasets/{config['Dataset']}_multimodal_filtered_maxlen{max_len}.csv\"\n",
    "\n",
    "    dna_dir = f\"embeddings/{config['Dataset']}/DNA/maxlen{max_len}\"\n",
    "    rna_dir = f\"embeddings/{config['Dataset']}/RNA/maxlen{max_len}\"\n",
    "    prot_dir = f\"embeddings/{config['Dataset']}/Protein/maxlen{max_len}\"\n",
    "\n",
    "    need_filtered_csv = not os.path.exists(filtered_csv)\n",
    "    need_dna = not _has_any_embeddings(dna_dir)\n",
    "    need_rna = not _has_any_embeddings(rna_dir)\n",
    "    need_prot = not _has_any_embeddings(prot_dir)\n",
    "    need_embeddings = need_dna or need_rna or need_prot\n",
    "\n",
    "    if need_filtered_csv or need_embeddings:\n",
    "        print(\"Embeddings and/or filtered CSV not found, calculating...\")\n",
    "        for modality in (\"DNA\", \"RNA\", \"Protein\"):\n",
    "            calculate_embeddings(\n",
    "                dataset=config[\"Dataset\"],\n",
    "                modality=modality,\n",
    "                device=config[\"device\"],\n",
    "                max_len=max_len,\n",
    "            )\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training Configuration (Multimodal):\")\n",
    "    print(f\"  Config name: {config.get('name', name)}\")\n",
    "    print(f\"  Dataset: {config['Dataset']}\")\n",
    "    print(f\"  Max Len (filter): {max_len}\")\n",
    "    print(f\"  Fusion: {config.get('fusion_type', 'concat')}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Device: {config['device']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nLoading data...\")\n",
    "    train_loader, val_loader, test_loader = get_multimodal_loaders(\n",
    "        config[\"Dataset\"],\n",
    "        batch_size=batch_size,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = build_model(config)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "    print(\"\\nInitializing trainer...\")\n",
    "    trainer = ClassificationTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=config[\"device\"],\n",
    "        save_dir=f\"./plots/{config.get('name', name)}/{config['Dataset']}\",\n",
    "    )\n",
    "\n",
    "    # entropy reg for MIL (nested config with backwards-compatible fallback)\n",
    "    lam_entropy = None\n",
    "    if isinstance(config.get(\"trainer\", None), dict):\n",
    "        lam_entropy = config[\"trainer\"].get(\"lam_entropy\", None)\n",
    "    if lam_entropy is None:\n",
    "        lam_entropy = config.get(\"lam_entropy\", None)\n",
    "\n",
    "    if lam_entropy is not None:\n",
    "        trainer.lam_entropy = float(lam_entropy)\n",
    "        if trainer.lam_entropy > 0:\n",
    "            print(f\"Using MIL entropy regularization: lam_entropy={trainer.lam_entropy}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    trainer.train(epochs=epochs)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training completed!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train multimodal model (DNA+RNA+Protein)\")\n",
    "    parser.add_argument(\n",
    "        \"--name\",\n",
    "        type=str,\n",
    "        default=\"fusion_mil\",\n",
    "        help=\"Config file name (without .yml). Options: fusion_concat, fusion_mil, fusion_xattn\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"ecoli_proteins\",\n",
    "        help=(\n",
    "            \"Dataset to use. Default: fungal_expression. Options: \"\n",
    "            \"'mrna_stability', 'ecoli_proteins', 'cov_vaccine_degradation', 'fungal_expression'\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-len\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help=\"Filter threshold: keep only sequences with raw length <= max_len before embedding/training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Batch size.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"Number of training epochs.\",\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    main(\n",
    "        name=args.name,\n",
    "        dataset=args.dataset,\n",
    "        max_len=args.max_len,\n",
    "        batch_size=args.batch_size,\n",
    "        epochs=args.epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0fe5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from data.dataloaders import get_multimodal_loaders\n",
    "from utils.load_config import load_config\n",
    "from utils.calculate_embeddings import calculate_embeddings\n",
    "from models.multimodel import build_model\n",
    "from trainer import RegressionTrainer\n",
    "\n",
    "\n",
    "def _has_any_embeddings(emb_dir) -> bool:\n",
    "    if not os.path.isdir(emb_dir):\n",
    "        return False\n",
    "    try:\n",
    "        for fn in os.listdir(emb_dir):\n",
    "            if fn.endswith(\".pt\"):\n",
    "                return True\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def main(name: str, dataset: str, max_len: int, batch_size: int, epochs: int):\n",
    "    config = load_config(f\"{name}.yml\")\n",
    "    config[\"Dataset\"] = dataset\n",
    "    config[\"device\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    max_len = int(max_len)\n",
    "\n",
    "    filtered_csv = f\"data/datasets/{config['Dataset']}_multimodal_filtered_maxlen{max_len}.csv\"\n",
    "\n",
    "    dna_dir = f\"embeddings/{config['Dataset']}/DNA/maxlen{max_len}\"\n",
    "    rna_dir = f\"embeddings/{config['Dataset']}/RNA/maxlen{max_len}\"\n",
    "    prot_dir = f\"embeddings/{config['Dataset']}/Protein/maxlen{max_len}\"\n",
    "\n",
    "    need_filtered_csv = not os.path.exists(filtered_csv)\n",
    "    need_dna = not _has_any_embeddings(dna_dir)\n",
    "    need_rna = not _has_any_embeddings(rna_dir)\n",
    "    need_prot = not _has_any_embeddings(prot_dir)\n",
    "    need_embeddings = need_dna or need_rna or need_prot\n",
    "\n",
    "    if need_filtered_csv or need_embeddings:\n",
    "        print(\"Embeddings and/or filtered CSV not found, calculating...\")\n",
    "        for modality in (\"DNA\", \"RNA\", \"Protein\"):\n",
    "            calculate_embeddings(\n",
    "                dataset=config[\"Dataset\"],\n",
    "                modality=modality,\n",
    "                device=config[\"device\"],\n",
    "                max_len=max_len,\n",
    "            )\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training Configuration (Multimodal):\")\n",
    "    print(f\"  Config name: {config.get('name', name)}\")\n",
    "    print(f\"  Dataset: {config['Dataset']}\")\n",
    "    print(f\"  Max Len (filter): {max_len}\")\n",
    "    print(f\"  Fusion: {config.get('fusion_type', 'concat')}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Device: {config['device']}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nLoading data...\")\n",
    "    train_loader, val_loader, test_loader = get_multimodal_loaders(\n",
    "        config[\"Dataset\"],\n",
    "        batch_size=batch_size,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = build_model(config)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "    print(\"\\nInitializing trainer...\")\n",
    "    trainer = ClassificationTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=config[\"device\"],\n",
    "        save_dir=f\"./plots/{config.get('name', name)}/{config['Dataset']}\",\n",
    "    )\n",
    "\n",
    "    # entropy reg for MIL (nested config with backwards-compatible fallback)\n",
    "    lam_entropy = None\n",
    "    if isinstance(config.get(\"trainer\", None), dict):\n",
    "        lam_entropy = config[\"trainer\"].get(\"lam_entropy\", None)\n",
    "    if lam_entropy is None:\n",
    "        lam_entropy = config.get(\"lam_entropy\", None)\n",
    "\n",
    "    if lam_entropy is not None:\n",
    "        trainer.lam_entropy = float(lam_entropy)\n",
    "        if trainer.lam_entropy > 0:\n",
    "            print(f\"Using MIL entropy regularization: lam_entropy={trainer.lam_entropy}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    trainer.train(epochs=epochs)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training completed!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train multimodal model (DNA+RNA+Protein)\")\n",
    "    parser.add_argument(\n",
    "        \"--name\",\n",
    "        type=str,\n",
    "        default=\"fusion_xattn\",\n",
    "        help=\"Config file name (without .yml). Options: fusion_concat, fusion_mil, fusion_xattn\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"ecoli_proteins\",\n",
    "        help=(\n",
    "            \"Dataset to use. Default: fungal_expression. Options: \"\n",
    "            \"'mrna_stability', 'ecoli_proteins', 'cov_vaccine_degradation', 'fungal_expression'\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-len\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help=\"Filter threshold: keep only sequences with raw length <= max_len before embedding/training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Batch size.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"Number of training epochs.\",\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    main(\n",
    "        name=args.name,\n",
    "        dataset=args.dataset,\n",
    "        max_len=args.max_len,\n",
    "        batch_size=args.batch_size,\n",
    "        epochs=args.epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e5a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0188625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83ddb166",
   "metadata": {},
   "source": [
    "# Uni Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05703c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Configuration:\n",
      "  Name: uni_rna\n",
      "  Dataset: ecoli_proteins\n",
      "  Modality: RNA\n",
      "  Max Len (filter): 1024\n",
      "============================================================\n",
      "\n",
      "Loading data...\n",
      "3073 742 721\n",
      "3073 742 721\n",
      "\n",
      "Initializing model...\n",
      "UnimodalClassificationModel(\n",
      "  (net): TextCNNHead(\n",
      "    (project): Linear(in_features=640, out_features=640, bias=True)\n",
      "    (convs): ModuleList(\n",
      "      (0): Conv1d(640, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (1): Conv1d(640, 100, kernel_size=(4,), stride=(1,), padding=(2,))\n",
      "      (2): Conv1d(640, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    )\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (fc): Linear(in_features=300, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 1179443\n",
      "Trainable parameters: 1179443\n",
      "Non-trainable parameters: 0\n",
      "\n",
      "Initializing trainer...\n",
      "\n",
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "Starting classification training for 500 epochs\n",
      "Device: cpu\n",
      "Classes: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|███▍                             | 10/97 [00:15<02:12,  1.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 156\u001b[0m\n\u001b[1;32m    149\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--max-len\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    152\u001b[0m     default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m    153\u001b[0m     help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilter threshold: keep only sequences with raw length <= max_len before embedding/training.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    155\u001b[0m args, _ \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()\n\u001b[0;32m--> 156\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 125\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(name, dataset, max_len)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 125\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Computational_Sciences_Engineering/COMP541_DeepLearniing/Comp541-Project-mserhat1-patch-6/trainer.py:464\u001b[0m, in \u001b[0;36mClassificationTrainer.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClasses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 464\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loader)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/Computational_Sciences_Engineering/COMP541_DeepLearniing/Comp541-Project-mserhat1-patch-6/trainer.py:387\u001b[0m, in \u001b[0;36mClassificationTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(embeddings, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m    385\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(logits, labels\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m--> 387\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    390\u001b[0m bs \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from data.dataloaders import get_loaders\n",
    "from data.subsampler.subsample import subsample_loader\n",
    "from utils.load_config import load_config\n",
    "from utils.calculate_embeddings import calculate_embeddings\n",
    "from models.unimodel import build_model\n",
    "from trainer import RegressionTrainer, ClassificationTrainer\n",
    "\n",
    "\n",
    "def main(name, dataset, max_len):\n",
    "    config = load_config(f\"{name}.yml\")\n",
    "    config[\"task\"] = \"classification\"\n",
    "    config[\"num_classes\"] = 3\n",
    "    config[\"Dataset\"] = dataset\n",
    "    config[\"device\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    max_len = int(max_len)\n",
    "\n",
    "    filtered_csv = f\"data/datasets/{config['Dataset']}_multimodal_filtered_maxlen{max_len}.csv\"\n",
    "    emb_dir = f\"embeddings/{config['Dataset']}/{config['modality']}/maxlen{max_len}\"\n",
    "\n",
    "    need_embeddings = not os.path.exists(os.path.join(emb_dir, \"seq1.pt\"))\n",
    "    need_filtered_csv = not os.path.exists(filtered_csv)\n",
    "\n",
    "    # Paths\n",
    "    filtered_csv = f\"data/datasets/{config['Dataset']}_multimodal_filtered_maxlen{max_len}.csv\"\n",
    "    emb_dir = f\"embeddings/{config['Dataset']}/{config['modality']}/maxlen{max_len}\"\n",
    "\n",
    "    need_filtered_csv = not os.path.exists(filtered_csv)\n",
    "    need_embeddings = not os.path.exists(emb_dir)\n",
    "\n",
    "    # 🔥 If CSV needs to be regenerated, embeddings are INVALID\n",
    "    if need_filtered_csv and os.path.exists(emb_dir):\n",
    "        print(\"Filtered CSV missing → removing stale embeddings\")\n",
    "        shutil.rmtree(emb_dir)\n",
    "\n",
    "    # 🔥 If embeddings exist but CSV was recreated earlier → force rebuild\n",
    "    if not need_filtered_csv and os.path.exists(emb_dir):\n",
    "        df = pd.read_csv(filtered_csv)\n",
    "        expected_ids = set(df[\"id\"].astype(str))\n",
    "        existing_ids = {\n",
    "            f.replace(\".pt\", \"\") for f in os.listdir(emb_dir)\n",
    "            if f.endswith(\".pt\")\n",
    "        }\n",
    "\n",
    "        if not expected_ids.issubset(existing_ids):\n",
    "            print(\"Embedding mismatch detected → removing stale embeddings\")\n",
    "            shutil.rmtree(emb_dir)\n",
    "            need_embeddings = True\n",
    "\n",
    "    if need_embeddings or need_filtered_csv:\n",
    "        print(\"Recomputing embeddings...\")\n",
    "        calculate_embeddings(\n",
    "            dataset=config[\"Dataset\"],\n",
    "            modality=config[\"modality\"],\n",
    "            device=config[\"device\"],\n",
    "            max_len=max_len,\n",
    "        )\n",
    "\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training Configuration:\")\n",
    "    print(f\"  Name: {config['name']}\")\n",
    "    print(f\"  Dataset: {config['Dataset']}\")\n",
    "    print(f\"  Modality: {config['modality']}\")\n",
    "    print(f\"  Max Len (filter): {max_len}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nLoading data...\")\n",
    "    train_loader, val_loader, test_loader = get_loaders(\n",
    "        config[\"Dataset\"],\n",
    "        32,\n",
    "        modality=config[\"modality\"],\n",
    "        max_len=max_len,\n",
    "    )\n",
    "    \n",
    "    print(\n",
    "        len(train_loader.dataset),\n",
    "        len(val_loader.dataset),\n",
    "        len(test_loader.dataset)\n",
    "    )\n",
    "\n",
    "    train_loader = subsample_loader(train_loader, fraction=1)\n",
    "    val_loader   = subsample_loader(val_loader, fraction=1)\n",
    "    test_loader  = subsample_loader(test_loader, fraction=1)\n",
    "    \n",
    "    print(\n",
    "        len(train_loader.dataset),\n",
    "        len(val_loader.dataset),\n",
    "        len(test_loader.dataset)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = build_model(config)\n",
    "    \n",
    "    print(model)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "    print(\"\\nInitializing trainer...\")\n",
    "    \n",
    "    trainer = ClassificationTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=config[\"device\"],\n",
    "        save_dir=f\"./plots/{config['name']}/{config['Dataset']}\",\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    trainer.train(epochs=500)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training completed!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train unimodal model\")\n",
    "    parser.add_argument(\n",
    "        \"--name\",\n",
    "        type=str,\n",
    "        default=\"uni_rna\",\n",
    "        help=\"Name of config file (without .yml extension) to use. Default: uni_rna\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"ecoli_proteins\",\n",
    "        help=(\n",
    "            \"Dataset to use. Default: fungal_expression. Options: \"\n",
    "            \"'mrna_stability', 'ecoli_proteins', 'cov_vaccine_degradation', 'fungal_expression'\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-len\",\n",
    "        type=int,\n",
    "        default=1024,\n",
    "        help=\"Filter threshold: keep only sequences with raw length <= max_len before embedding/training.\",\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "    main(args.name, args.dataset, args.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c7fa363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Configuration:\n",
      "  Name: uni_rna\n",
      "  Dataset: fungal_expression\n",
      "  Modality: RNA\n",
      "  Max Len (filter): 1024\n",
      "============================================================\n",
      "\n",
      "Loading data...\n",
      "2232 566 445\n",
      "2232 566 445\n",
      "\n",
      "Initializing model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 153\u001b[0m\n\u001b[1;32m    146\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--max-len\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    149\u001b[0m     default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m    150\u001b[0m     help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFilter threshold: keep only sequences with raw length <= max_len before embedding/training.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    151\u001b[0m )\n\u001b[1;32m    152\u001b[0m args, _ \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()\n\u001b[0;32m--> 153\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 99\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(name, dataset, max_len)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset),\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mlen\u001b[39m(val_loader\u001b[38;5;241m.\u001b[39mdataset),\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mlen\u001b[39m(test_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInitializing model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    102\u001b[0m trainable_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad)\n",
      "File \u001b[0;32m~/Desktop/Computational_Sciences_Engineering/COMP541_DeepLearniing/Comp541-Project-mserhat1-patch-6/models/unimodel.py:38\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UnimodalClassificationModel(\n\u001b[1;32m     33\u001b[0m         input_dim\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     34\u001b[0m         num_classes\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m]   \n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# 🔥 regression MUST output 1 value\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnimodalRegressionModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Computational_Sciences_Engineering/COMP541_DeepLearniing/Comp541-Project-mserhat1-patch-6/models/unimodel.py:7\u001b[0m, in \u001b[0;36mUnimodalRegressionModel.__init__\u001b[0;34m(self, input_dim)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m=\u001b[39m \u001b[43mTextCNNHead\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_classes'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from data.dataloaders import get_loaders\n",
    "from data.subsampler.subsample import subsample_loader\n",
    "from utils.load_config import load_config\n",
    "from utils.calculate_embeddings import calculate_embeddings\n",
    "from models.unimodel import build_model\n",
    "from trainer import RegressionTrainer, ClassificationTrainer\n",
    "\n",
    "\n",
    "def main(name, dataset, max_len):\n",
    "    config = load_config(f\"{name}.yml\")\n",
    "    config[\"task\"] = \"regression\"\n",
    "    #config[\"num_classes\"] = 3\n",
    "    config[\"Dataset\"] = dataset\n",
    "    config[\"device\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    max_len = int(max_len)\n",
    "\n",
    "    filtered_csv = f\"data/datasets/{config['Dataset']}_multimodal_filtered_maxlen{max_len}.csv\"\n",
    "    emb_dir = f\"embeddings/{config['Dataset']}/{config['modality']}/maxlen{max_len}\"\n",
    "\n",
    "    need_embeddings = not os.path.exists(os.path.join(emb_dir, \"seq1.pt\"))\n",
    "    need_filtered_csv = not os.path.exists(filtered_csv)\n",
    "\n",
    "    # Paths\n",
    "    filtered_csv = f\"data/datasets/{config['Dataset']}_multimodal_filtered_maxlen{max_len}.csv\"\n",
    "    emb_dir = f\"embeddings/{config['Dataset']}/{config['modality']}/maxlen{max_len}\"\n",
    "\n",
    "    need_filtered_csv = not os.path.exists(filtered_csv)\n",
    "    need_embeddings = not os.path.exists(emb_dir)\n",
    "\n",
    "    # 🔥 If CSV needs to be regenerated, embeddings are INVALID\n",
    "    if need_filtered_csv and os.path.exists(emb_dir):\n",
    "        print(\"Filtered CSV missing → removing stale embeddings\")\n",
    "        shutil.rmtree(emb_dir)\n",
    "\n",
    "    # 🔥 If embeddings exist but CSV was recreated earlier → force rebuild\n",
    "    if not need_filtered_csv and os.path.exists(emb_dir):\n",
    "        df = pd.read_csv(filtered_csv)\n",
    "        expected_ids = set(df[\"id\"].astype(str))\n",
    "        existing_ids = {\n",
    "            f.replace(\".pt\", \"\") for f in os.listdir(emb_dir)\n",
    "            if f.endswith(\".pt\")\n",
    "        }\n",
    "\n",
    "        if not expected_ids.issubset(existing_ids):\n",
    "            print(\"Embedding mismatch detected → removing stale embeddings\")\n",
    "            shutil.rmtree(emb_dir)\n",
    "            need_embeddings = True\n",
    "\n",
    "    if need_embeddings or need_filtered_csv:\n",
    "        print(\"Recomputing embeddings...\")\n",
    "        calculate_embeddings(\n",
    "            dataset=config[\"Dataset\"],\n",
    "            modality=config[\"modality\"],\n",
    "            device=config[\"device\"],\n",
    "            max_len=max_len,\n",
    "        )\n",
    "\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Training Configuration:\")\n",
    "    print(f\"  Name: {config['name']}\")\n",
    "    print(f\"  Dataset: {config['Dataset']}\")\n",
    "    print(f\"  Modality: {config['modality']}\")\n",
    "    print(f\"  Max Len (filter): {max_len}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nLoading data...\")\n",
    "    train_loader, val_loader, test_loader = get_loaders(\n",
    "        config[\"Dataset\"],\n",
    "        32,\n",
    "        modality=config[\"modality\"],\n",
    "        max_len=max_len,\n",
    "    )\n",
    "    \n",
    "    print(\n",
    "        len(train_loader.dataset),\n",
    "        len(val_loader.dataset),\n",
    "        len(test_loader.dataset)\n",
    "    )\n",
    "\n",
    "    train_loader = subsample_loader(train_loader, fraction=1)\n",
    "    val_loader   = subsample_loader(val_loader, fraction=1)\n",
    "    test_loader  = subsample_loader(test_loader, fraction=1)\n",
    "    \n",
    "    print(\n",
    "        len(train_loader.dataset),\n",
    "        len(val_loader.dataset),\n",
    "        len(test_loader.dataset)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = build_model(config)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params}\")\n",
    "\n",
    "    print(\"\\nInitializing trainer...\")\n",
    "    trainer = RegressionTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        device=config[\"device\"],\n",
    "        save_dir=f\"./plots/{config['name']}/{config['Dataset']}\",\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    trainer.train(epochs=500)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training completed!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train unimodal model\")\n",
    "    parser.add_argument(\n",
    "        \"--name\",\n",
    "        type=str,\n",
    "        default=\"uni_rna\",\n",
    "        help=\"Name of config file (without .yml extension) to use. Default: uni_rna\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"fungal_expression\",\n",
    "        help=(\n",
    "            \"Dataset to use. Default: fungal_expression. Options: \"\n",
    "            \"'mrna_stability', 'ecoli_proteins', 'cov_vaccine_degradation', 'fungal_expression'\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-len\",\n",
    "        type=int,\n",
    "        default=1024,\n",
    "        help=\"Filter threshold: keep only sequences with raw length <= max_len before embedding/training.\",\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "    main(args.name, args.dataset, args.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba2f5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
