{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94dc724d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙ Running on: CPU\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up one level: notebooks → project root\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Import all model components from your repo\n",
    "# ----------------------------------------------\n",
    "\n",
    "from models.dna_model import NucleotideTransformerEmbedder\n",
    "from models.rna_model import RNAFMEmbedder\n",
    "from models.protein_model import ESM2Embedder\n",
    "from models.text_model import TextEmbedder\n",
    "\n",
    "from models.projection_heads import ProjectionHead\n",
    "from models.fusion_concat import FusionConcat\n",
    "from models.fusion_mil import FusionMIL\n",
    "from models.fusion_xattn import FusionCrossAttention\n",
    "\n",
    "from models.prediction_head import TextCNNHead, MLPHead\n",
    "from models.lora_adapter import LoRAAdapter\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"⚙ Running on:\", device.upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9207a876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading public DNA model: zhihan1996/DNABERT-2-117M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sahandhassani/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/sahandhassani/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/7bce263b15377fc15361f52cfab88f8b586abda0/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at facebook/esm2_t30_150M_UR50D were not used when initializing EsmModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders initialized.\n"
     ]
    }
   ],
   "source": [
    "# Short toy sequences for testing\n",
    "\n",
    "dna_seq = \"ATGCGTACGTAGCTAGCTAGCTA\"\n",
    "rna_seq = \"AUGGCUACUGAACCUUAGCUGGAAA\"\n",
    "protein_seq = \"MKTLLIALAVAAALA\"\n",
    "text_info = \"This is a sample description of an mRNA sequence.\"\n",
    "\n",
    "# Instantiate encoders\n",
    "dna_enc = NucleotideTransformerEmbedder(max_len=200, device=device)\n",
    "rna_enc = RNAFMEmbedder(max_len=200, device=device)\n",
    "protein_enc = ESM2Embedder(max_len=200, device=device)\n",
    "text_enc = TextEmbedder(max_len=100, device=device)\n",
    "\n",
    "print(\"Encoders initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b3f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA embedding: torch.Size([200, 768])\n",
      "RNA embedding: torch.Size([200, 320])\n",
      "Protein embedding: torch.Size([200, 640])\n",
      "Text embedding: torch.Size([100, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    dna_emb = dna_enc(dna_seq)\n",
    "    rna_emb = rna_enc(rna_seq)\n",
    "    protein_emb = protein_enc(protein_seq)\n",
    "    text_emb = text_enc(text_info)\n",
    "\n",
    "print(\"DNA embedding:\", dna_emb.shape)\n",
    "print(\"RNA embedding:\", rna_emb.shape)\n",
    "print(\"Protein embedding:\", protein_emb.shape)\n",
    "print(\"Text embedding:\", text_emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0db4bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected shapes:\n",
      "DNA: torch.Size([200, 256])\n",
      "RNA: torch.Size([200, 256])\n",
      "Protein: torch.Size([200, 256])\n",
      "Text: torch.Size([100, 256])\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 256\n",
    "\n",
    "proj_dna = ProjectionHead(input_dim=dna_emb.shape[-1], output_dim=latent_dim).to(device)\n",
    "proj_rna = ProjectionHead(input_dim=rna_emb.shape[-1], output_dim=latent_dim).to(device)\n",
    "proj_prot = ProjectionHead(input_dim=protein_emb.shape[-1], output_dim=latent_dim).to(device)\n",
    "proj_text = ProjectionHead(input_dim=text_emb.shape[-1], output_dim=latent_dim).to(device)\n",
    "\n",
    "dna_z = proj_dna(dna_emb)\n",
    "rna_z = proj_rna(rna_emb)\n",
    "prot_z = proj_prot(protein_emb)\n",
    "text_z = proj_text(text_emb)\n",
    "\n",
    "print(\"Projected shapes:\")\n",
    "print(\"DNA:\", dna_z.shape)\n",
    "print(\"RNA:\", rna_z.shape)\n",
    "print(\"Protein:\", prot_z.shape)\n",
    "print(\"Text:\", text_z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1612589b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused concat shape: torch.Size([200, 768])\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 256\n",
    "\n",
    "# Dimensions after your ProjectionHead\n",
    "dDNA = dna_z.shape[-1]    # = 256\n",
    "dRNA = rna_z.shape[-1]    # = 256\n",
    "dProt = prot_z.shape[-1]  # = 256\n",
    "\n",
    "fusion = FusionConcat(\n",
    "    dDNA=dDNA,\n",
    "    dRNA=dRNA,\n",
    "    dProt=dProt,\n",
    "    dDNA_proj=256     # or 128 to reduce DNA dominance\n",
    ")\n",
    "\n",
    "fused = fusion(dna_z, rna_z, prot_z)\n",
    "print(\"Fused concat shape:\", fused.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce57bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused (MIL) shape: torch.Size([200, 256])\n"
     ]
    }
   ],
   "source": [
    "dDNA = dna_z.shape[-1]    # 256\n",
    "dRNA = rna_z.shape[-1]    # 256\n",
    "dProt = prot_z.shape[-1]  # 256\n",
    "\n",
    "fusion = FusionMIL(\n",
    "    dDNA=dDNA,\n",
    "    dRNA=dRNA,\n",
    "    dProt=dProt,\n",
    "    d_model=256,      # shared dimension\n",
    "    d_attn=128        # gating attention dimension\n",
    ")\n",
    "\n",
    "fused = fusion(dna_z, rna_z, prot_z)\n",
    "\n",
    "print(\"Fused (MIL) shape:\", fused.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29dc00b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused (xAttn) shape: torch.Size([200, 256])\n"
     ]
    }
   ],
   "source": [
    "fusion = FusionCrossAttention(\n",
    "    dDNA=dDNA,\n",
    "    dRNA=dRNA,\n",
    "    dProt=dProt,\n",
    "    d_model=latent_dim,\n",
    "    num_heads=4\n",
    ")\n",
    "\n",
    "fused = fusion(dna_z, rna_z, prot_z)\n",
    "print(\"Fused (xAttn) shape:\", fused.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca18aeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape (TextCNN): torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "pred_head = TextCNNHead(embed_dim=latent_dim, num_classes=1).to(device)\n",
    "prediction = pred_head(fused)\n",
    "\n",
    "print(\"Prediction shape (TextCNN):\", prediction.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14170c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape (MLP): torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "pred_head = MLPHead(input_dim=latent_dim, num_classes=1).to(device)\n",
    "prediction = pred_head(fused)\n",
    "\n",
    "print(\"Prediction shape (MLP):\", prediction.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa8a5044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA output shape: torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(latent_dim, latent_dim)\n",
    "lora = LoRAAdapter(linear, rank=8)\n",
    "\n",
    "x = torch.randn(10, latent_dim)\n",
    "y = lora(x)\n",
    "\n",
    "print(\"LoRA output shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82829710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== SUMMARY ======\n",
      "DNA emb: torch.Size([200, 768])\n",
      "RNA emb: torch.Size([200, 320])\n",
      "Protein emb: torch.Size([200, 640])\n",
      "Text emb: torch.Size([100, 768])\n",
      "After projection: torch.Size([200, 256])\n",
      "Fused representation: torch.Size([200, 256])\n",
      "Final prediction: torch.Size([1])\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n====== SUMMARY ======\")\n",
    "print(\"DNA emb:\", dna_emb.shape)\n",
    "print(\"RNA emb:\", rna_emb.shape)\n",
    "print(\"Protein emb:\", protein_emb.shape)\n",
    "print(\"Text emb:\", text_emb.shape)\n",
    "\n",
    "print(\"After projection:\", dna_z.shape)\n",
    "print(\"Fused representation:\", fused.shape)\n",
    "print(\"Final prediction:\", prediction.shape)\n",
    "print(\"======================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
